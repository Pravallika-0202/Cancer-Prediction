{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPkYdNCxTiorFEjNhKgWpDX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pravallika-0202/Cancer-Prediction/blob/main/SparkFlow_Ecommerce_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WamQ5eUZ5mo",
        "outputId": "8d38f17c-a3c1-4e4d-d28f-e5775e8e353c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (4.0.2)\n",
            "Requirement already satisfied: py4j<0.10.9.10,>=0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SparkFlow_Ecommerce_Pipeline\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "\n",
        "print(\"Spark Version:\", spark.version)\n",
        "print(\"Master:\", sc.master)\n",
        "print(\"App Name:\", sc.appName)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7r95wbTgZ-Um",
        "outputId": "0194c1ce-f215-4c8f-e464-a8f03e010eea"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark Version: 4.0.2\n",
            "Master: local[*]\n",
            "App Name: SparkFlow_Ecommerce_Pipeline\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Environment Choice: Google Colab with PySpark\n",
        "\n",
        "This project uses Google Colab with PySpark installed via pip. Spark runs in local mode (local[*]), simulating the Driver and Executors on a single machine. This setup allows development and demonstration of Spark architecture, transformations, Spark SQL, and distributed processing concepts without requiring cluster provisioning.\n",
        "\n",
        "Spark Version: 4.0.2\n",
        "Execution Mode: local[*]"
      ],
      "metadata": {
        "id": "lIUqO2qvaWvv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What role does the Driver play?\n",
        "\n",
        "The Driver is the control center of a Spark application.\n",
        "\n",
        "It:\n",
        "\n",
        "Creates the SparkSession\n",
        "\n",
        "Defines transformations and builds the DAG (Directed Acyclic Graph)\n",
        "\n",
        "Communicates with the Cluster Manager to request resources\n",
        "\n",
        "Schedules tasks to Executors\n",
        "\n",
        "Collects results from Executors when actions are triggered\n",
        "\n",
        "In local mode (local[*]), the notebook process acts as the Driver."
      ],
      "metadata": {
        "id": "bQw7dEkdarPR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What are Executors, and why are they long-lived?\n",
        "\n",
        "Executors are worker processes that:\n",
        "\n",
        "Execute tasks sent by the Driver\n",
        "\n",
        "Store data in memory (caching/persisting)\n",
        "\n",
        "Handle shuffle operations\n",
        "\n",
        "Return results back to the Driver\n",
        "\n",
        "They are long-lived because:\n",
        "\n",
        "Starting/stopping processes repeatedly is expensive\n",
        "\n",
        "Keeping executors alive allows caching intermediate results\n",
        "\n",
        "Reduces overhead and improves performance\n",
        "\n",
        "In local mode, executor threads run within the same machine."
      ],
      "metadata": {
        "id": "5J3ZKCxBatr_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is a Cluster Manager, and how does Spark interact with it?\n",
        "\n",
        "A Cluster Manager is responsible for allocating computing resources.\n",
        "\n",
        "Examples:\n",
        "\n",
        "Standalone\n",
        "\n",
        "YARN\n",
        "\n",
        "Kubernetes\n",
        "\n",
        "Databricks-managed clusters\n",
        "\n",
        "Spark Driver requests resources (CPU, memory) from the Cluster Manager.\n",
        "The Cluster Manager launches Executors on available nodes.\n",
        "\n",
        "In this Colab setup:\n",
        "There is no external cluster manager — Spark runs in local mode."
      ],
      "metadata": {
        "id": "np-j4hEcaz_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "data = spark.range(1, 1000000)\n",
        "\n",
        "transformed = data.filter(F.col(\"id\") % 2 == 0)\n",
        "\n",
        "print(\"Transformation defined. No execution yet.\")\n",
        "\n",
        "# Now trigger execution\n",
        "count_result = transformed.count()\n",
        "\n",
        "print(\"Count of even numbers:\", count_result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VKH88w9bWSb",
        "outputId": "b3a682f0-a13d-409b-dd93-d2d03ddcdda9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformation defined. No execution yet.\n",
            "Count of even numbers: 499999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "base_path = \"/content/sparkflow_raw\"\n",
        "os.makedirs(base_path, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "ztdwPZCocIrn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customers_data = \"\"\"customer_id,first_name,last_name,email,signup_ts,country\n",
        "C001,John,Doe,john@example.com,2023-01-10 10:00:00,Australia\n",
        "C002,Jane,Smith,jane@example.com,2023-03-15 12:30:00,Australia\n",
        "C003,Mike,Brown,mike@example.com,2022-11-05 09:15:00,USA\n",
        "C004,Sarah,Wilson,sarah@example.com,2024-02-20 14:45:00,UK\n",
        "\"\"\"\n",
        "\n",
        "with open(f\"{base_path}/customers.csv\", \"w\") as f:\n",
        "    f.write(customers_data)\n"
      ],
      "metadata": {
        "id": "1HkkUZP3cOEi"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "products_data = \"\"\"product_id,product_name,category,unit_price\n",
        "P001,Laptop,Electronics,1200.50\n",
        "P002,Headphones,Electronics,150.00\n",
        "P003,Coffee Machine,Home Appliances,300.75\n",
        "P004,Desk Chair,Furniture,220.40\n",
        "\"\"\"\n",
        "\n",
        "with open(f\"{base_path}/products.csv\", \"w\") as f:\n",
        "    f.write(products_data)\n"
      ],
      "metadata": {
        "id": "SsjSHN7acRbX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "orders_data = \"\"\"\n",
        "{\"order_id\":\"O001\",\"customer_id\":\"C001\",\"product_id\":\"P001\",\"quantity\":\"1\",\"order_ts\":\"2023-02-01 11:00:00\",\"platform\":\"Web\"}\n",
        "{\"order_id\":\"O002\",\"customer_id\":\"C002\",\"product_id\":\"P002\",\"quantity\":\"2\",\"order_ts\":\"2023-04-01 15:20:00\",\"platform\":\"Mobile\"}\n",
        "{\"order_id\":\"O003\",\"customer_id\":\"C001\",\"product_id\":\"P003\",\"quantity\":\"1\",\"order_ts\":\"2023-05-10 09:00:00\",\"platform\":\"Web\"}\n",
        "{\"order_id\":\"O004\",\"customer_id\":\"C003\",\"product_id\":\"P004\",\"quantity\":\"3\",\"order_ts\":\"2022-12-15 16:40:00\",\"platform\":\"Web\"}\n",
        "\"\"\"\n",
        "\n",
        "with open(f\"{base_path}/orders.json\", \"w\") as f:\n",
        "    f.write(orders_data)\n"
      ],
      "metadata": {
        "id": "2zh4vgmdcUqn"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "customers_schema = StructType([\n",
        "    StructField(\"customer_id\", StringType(), True),\n",
        "    StructField(\"first_name\", StringType(), True),\n",
        "    StructField(\"last_name\", StringType(), True),\n",
        "    StructField(\"email\", StringType(), True),\n",
        "    StructField(\"signup_ts\", StringType(), True),\n",
        "    StructField(\"country\", StringType(), True),\n",
        "])\n",
        "\n",
        "products_schema = StructType([\n",
        "    StructField(\"product_id\", StringType(), True),\n",
        "    StructField(\"product_name\", StringType(), True),\n",
        "    StructField(\"category\", StringType(), True),\n",
        "    StructField(\"unit_price\", StringType(), True),\n",
        "])\n",
        "\n",
        "orders_schema = StructType([\n",
        "    StructField(\"order_id\", StringType(), True),\n",
        "    StructField(\"customer_id\", StringType(), True),\n",
        "    StructField(\"product_id\", StringType(), True),\n",
        "    StructField(\"quantity\", StringType(), True),\n",
        "    StructField(\"order_ts\", StringType(), True),\n",
        "    StructField(\"platform\", StringType(), True),\n",
        "])\n"
      ],
      "metadata": {
        "id": "HBLgth3_cZvx"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customers_raw = spark.read \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .schema(customers_schema) \\\n",
        "    .csv(f\"{base_path}/customers.csv\")\n",
        "\n",
        "products_raw = spark.read \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .schema(products_schema) \\\n",
        "    .csv(f\"{base_path}/products.csv\")\n",
        "\n",
        "orders_raw = spark.read \\\n",
        "    .schema(orders_schema) \\\n",
        "    .json(f\"{base_path}/orders.json\")\n"
      ],
      "metadata": {
        "id": "DReNzjLpcihe"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customers_raw.printSchema()\n",
        "customers_raw.show()\n",
        "\n",
        "products_raw.printSchema()\n",
        "products_raw.show()\n",
        "\n",
        "orders_raw.printSchema()\n",
        "orders_raw.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cszlpjdVcoI0",
        "outputId": "fb7bba3b-ab63-4983-be8c-48980e2bb7bb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- first_name: string (nullable = true)\n",
            " |-- last_name: string (nullable = true)\n",
            " |-- email: string (nullable = true)\n",
            " |-- signup_ts: string (nullable = true)\n",
            " |-- country: string (nullable = true)\n",
            "\n",
            "+-----------+----------+---------+-----------------+-------------------+---------+\n",
            "|customer_id|first_name|last_name|            email|          signup_ts|  country|\n",
            "+-----------+----------+---------+-----------------+-------------------+---------+\n",
            "|       C001|      John|      Doe| john@example.com|2023-01-10 10:00:00|Australia|\n",
            "|       C002|      Jane|    Smith| jane@example.com|2023-03-15 12:30:00|Australia|\n",
            "|       C003|      Mike|    Brown| mike@example.com|2022-11-05 09:15:00|      USA|\n",
            "|       C004|     Sarah|   Wilson|sarah@example.com|2024-02-20 14:45:00|       UK|\n",
            "+-----------+----------+---------+-----------------+-------------------+---------+\n",
            "\n",
            "root\n",
            " |-- product_id: string (nullable = true)\n",
            " |-- product_name: string (nullable = true)\n",
            " |-- category: string (nullable = true)\n",
            " |-- unit_price: string (nullable = true)\n",
            "\n",
            "+----------+--------------+---------------+----------+\n",
            "|product_id|  product_name|       category|unit_price|\n",
            "+----------+--------------+---------------+----------+\n",
            "|      P001|        Laptop|    Electronics|   1200.50|\n",
            "|      P002|    Headphones|    Electronics|    150.00|\n",
            "|      P003|Coffee Machine|Home Appliances|    300.75|\n",
            "|      P004|    Desk Chair|      Furniture|    220.40|\n",
            "+----------+--------------+---------------+----------+\n",
            "\n",
            "root\n",
            " |-- order_id: string (nullable = true)\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- product_id: string (nullable = true)\n",
            " |-- quantity: string (nullable = true)\n",
            " |-- order_ts: string (nullable = true)\n",
            " |-- platform: string (nullable = true)\n",
            "\n",
            "+--------+-----------+----------+--------+-------------------+--------+\n",
            "|order_id|customer_id|product_id|quantity|           order_ts|platform|\n",
            "+--------+-----------+----------+--------+-------------------+--------+\n",
            "|    O001|       C001|      P001|       1|2023-02-01 11:00:00|     Web|\n",
            "|    O002|       C002|      P002|       2|2023-04-01 15:20:00|  Mobile|\n",
            "|    O003|       C001|      P003|       1|2023-05-10 09:00:00|     Web|\n",
            "|    O004|       C003|      P004|       3|2022-12-15 16:40:00|     Web|\n",
            "+--------+-----------+----------+--------+-------------------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data was read using Spark’s distributed readers\n",
        "\n",
        "Explicit schemas were defined to avoid schema inference cost\n",
        "\n",
        "Raw datasets are preserved without transformations\n",
        "\n",
        "Spark automatically partitions data internally"
      ],
      "metadata": {
        "id": "r1laiSVhc6MU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "orders_rdd = orders_raw.rdd\n",
        "\n",
        "print(\"Number of partitions:\", orders_rdd.getNumPartitions())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BryHLL50c8zg",
        "outputId": "f26d6c4b-285b-4ce4-fac4-e725ecfc10e2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of partitions: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def safe_int(x):\n",
        "    try:\n",
        "        return int(x)\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "quantity_rdd = orders_rdd.map(lambda row: safe_int(row[\"quantity\"]))\n",
        "\n",
        "quantity_rdd.collect()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJc5QKoWdHTo",
        "outputId": "c7faed82-fa1b-4720-fd38-9afdd3cd86bb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 1, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "positive_qty_rdd = quantity_rdd.filter(lambda q: q > 0)\n",
        "\n",
        "positive_qty_rdd.collect()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLGWX2E0dKiI",
        "outputId": "e2206337-88eb-45d6-e2ff-aed2d52554cb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 1, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_quantity = positive_qty_rdd.reduce(lambda a, b: a + b)\n",
        "\n",
        "print(\"Total quantity across all orders:\", total_quantity)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpxggE6rdO_d",
        "outputId": "4b42785d-98ad-4933-ede6-881a7c7e793e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total quantity across all orders: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_orders = orders_rdd.map(lambda row: 1).reduce(lambda a, b: a + b)\n",
        "\n",
        "print(\"Total number of orders:\", total_orders)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O10iMqDwdSIG",
        "outputId": "dc50ef74-70dc-4e52-ee43-2f1e82adb6f1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of orders: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 4 – RDD Fundamentals\n",
        "\n",
        "In this step, the orders DataFrame was converted into an RDD using .rdd. This exposes Spark’s low-level distributed abstraction.\n",
        "\n",
        "The following RDD operations were demonstrated:\n",
        "\n",
        "map() – Applied a transformation to each record (extracting quantity).\n",
        "\n",
        "filter() – Filtered records based on a condition (positive quantities).\n",
        "\n",
        "reduce() – Aggregated values across partitions to compute totals.\n",
        "\n",
        "RDDs provide fine-grained control over distributed data processing and allow arbitrary transformations. However, they lack the Catalyst optimizer and Tungsten execution engine available in Spark SQL and DataFrames. As a result, RDD operations are generally less efficient and more verbose compared to DataFrame APIs."
      ],
      "metadata": {
        "id": "KnItN4MPdYsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Clean customers\n",
        "customers = (\n",
        "    customers_raw\n",
        "    .filter(F.col(\"customer_id\").isNotNull())\n",
        "    .withColumn(\"signup_ts\", F.to_timestamp(\"signup_ts\"))\n",
        "    .dropDuplicates([\"customer_id\"])\n",
        ")\n",
        "\n",
        "# Clean products\n",
        "products = (\n",
        "    products_raw\n",
        "    .filter(F.col(\"product_id\").isNotNull())\n",
        "    .withColumn(\"unit_price\", F.col(\"unit_price\").cast(\"double\"))\n",
        "    .dropDuplicates([\"product_id\"])\n",
        ")\n",
        "\n",
        "# Clean orders\n",
        "orders = (\n",
        "    orders_raw\n",
        "    .filter(F.col(\"order_id\").isNotNull())\n",
        "    .filter(F.col(\"customer_id\").isNotNull())\n",
        "    .filter(F.col(\"product_id\").isNotNull())\n",
        "    .withColumn(\"quantity\", F.col(\"quantity\").cast(\"int\"))\n",
        "    .withColumn(\"order_ts\", F.to_timestamp(\"order_ts\"))\n",
        "    .filter(F.col(\"quantity\") > 0)\n",
        "    .dropDuplicates([\"order_id\"])\n",
        ")\n"
      ],
      "metadata": {
        "id": "gFoVbyF1dpwP"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "orders.printSchema()\n",
        "orders.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPkEFDV_dtGe",
        "outputId": "43e46946-7ecb-42e8-bfdf-ad03e70dee78"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- order_id: string (nullable = true)\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- product_id: string (nullable = true)\n",
            " |-- quantity: integer (nullable = true)\n",
            " |-- order_ts: timestamp (nullable = true)\n",
            " |-- platform: string (nullable = true)\n",
            "\n",
            "+--------+-----------+----------+--------+-------------------+--------+\n",
            "|order_id|customer_id|product_id|quantity|           order_ts|platform|\n",
            "+--------+-----------+----------+--------+-------------------+--------+\n",
            "|    O001|       C001|      P001|       1|2023-02-01 11:00:00|     Web|\n",
            "|    O002|       C002|      P002|       2|2023-04-01 15:20:00|  Mobile|\n",
            "|    O003|       C001|      P003|       1|2023-05-10 09:00:00|     Web|\n",
            "|    O004|       C003|      P004|       3|2022-12-15 16:40:00|     Web|\n",
            "+--------+-----------+----------+--------+-------------------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fact_orders = (\n",
        "    orders.alias(\"o\")\n",
        "    .join(products.alias(\"p\"), on=\"product_id\", how=\"left\")\n",
        ")\n"
      ],
      "metadata": {
        "id": "P9p24dgPd98H"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fact_orders = (\n",
        "    fact_orders\n",
        "    .withColumn(\"order_value\", F.col(\"quantity\") * F.col(\"unit_price\"))\n",
        "    .withColumn(\"is_high_value_order\", F.col(\"order_value\") >= 500)\n",
        "    .withColumn(\"order_year\", F.year(\"order_ts\"))\n",
        ")\n"
      ],
      "metadata": {
        "id": "G3Ve1fjLeBMt"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fact_orders.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biW9L9qoeEa0",
        "outputId": "6fe234ca-105b-4173-f09d-9b7822047408"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------+-----------+--------+-------------------+--------+--------------+---------------+----------+-----------+-------------------+----------+\n",
            "|product_id|order_id|customer_id|quantity|           order_ts|platform|  product_name|       category|unit_price|order_value|is_high_value_order|order_year|\n",
            "+----------+--------+-----------+--------+-------------------+--------+--------------+---------------+----------+-----------+-------------------+----------+\n",
            "|      P001|    O001|       C001|       1|2023-02-01 11:00:00|     Web|        Laptop|    Electronics|    1200.5|     1200.5|               true|      2023|\n",
            "|      P002|    O002|       C002|       2|2023-04-01 15:20:00|  Mobile|    Headphones|    Electronics|     150.0|      300.0|              false|      2023|\n",
            "|      P003|    O003|       C001|       1|2023-05-10 09:00:00|     Web|Coffee Machine|Home Appliances|    300.75|     300.75|              false|      2023|\n",
            "|      P004|    O004|       C003|       3|2022-12-15 16:40:00|     Web|    Desk Chair|      Furniture|     220.4|      661.2|               true|      2022|\n",
            "+----------+--------+-----------+--------+-------------------+--------+--------------+---------------+----------+-----------+-------------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fact_orders.select(\"order_id\", \"customer_id\", \"order_value\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOXP2MRneHUf",
        "outputId": "62b2736f-f964-4d7e-b685-4d901c0e7ba8"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----------+-----------+\n",
            "|order_id|customer_id|order_value|\n",
            "+--------+-----------+-----------+\n",
            "|    O001|       C001|     1200.5|\n",
            "|    O002|       C002|      300.0|\n",
            "|    O003|       C001|     300.75|\n",
            "|    O004|       C003|      661.2|\n",
            "+--------+-----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "high_value_orders = fact_orders.filter(F.col(\"is_high_value_order\") == True)\n",
        "high_value_orders.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lojCx9iweK7j",
        "outputId": "ad6e3f0e-1fa5-4886-f8ca-6b1ef2eed69f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------+-----------+--------+-------------------+--------+------------+-----------+----------+-----------+-------------------+----------+\n",
            "|product_id|order_id|customer_id|quantity|           order_ts|platform|product_name|   category|unit_price|order_value|is_high_value_order|order_year|\n",
            "+----------+--------+-----------+--------+-------------------+--------+------------+-----------+----------+-----------+-------------------+----------+\n",
            "|      P001|    O001|       C001|       1|2023-02-01 11:00:00|     Web|      Laptop|Electronics|    1200.5|     1200.5|               true|      2023|\n",
            "|      P004|    O004|       C003|       3|2022-12-15 16:40:00|     Web|  Desk Chair|  Furniture|     220.4|      661.2|               true|      2022|\n",
            "+----------+--------+-----------+--------+-------------------+--------+------------+-----------+----------+-----------+-------------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yearly_revenue = (\n",
        "    fact_orders\n",
        "    .groupBy(\"order_year\")\n",
        "    .agg(\n",
        "        F.sum(\"order_value\").alias(\"total_revenue\"),\n",
        "        F.countDistinct(\"order_id\").alias(\"order_count\")\n",
        "    )\n",
        "    .orderBy(\"order_year\")\n",
        ")\n",
        "\n",
        "yearly_revenue.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J21SDZ0VeOVQ",
        "outputId": "2ca3604d-beac-48fe-f4ce-6a99ca811ccd"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+-----------+\n",
            "|order_year|total_revenue|order_count|\n",
            "+----------+-------------+-----------+\n",
            "|      2022|        661.2|          1|\n",
            "|      2023|      1801.25|          3|\n",
            "+----------+-------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 5 – DataFrame Transformations\n",
        "\n",
        "This stage converts raw datasets into cleaned analytical tables.\n",
        "\n",
        "Data quality improvements included:\n",
        "\n",
        "Removing null identifiers\n",
        "\n",
        "Casting numeric fields (quantity, unit_price)\n",
        "\n",
        "Converting timestamp fields to proper datetime types\n",
        "\n",
        "Removing duplicate records\n",
        "\n",
        "Orders were joined with products to create a fact table. Derived columns were created:\n",
        "\n",
        "order_value = quantity × unit_price\n",
        "\n",
        "is_high_value_order = Boolean flag for revenue ≥ 500\n",
        "\n",
        "order_year extracted from timestamp\n",
        "\n",
        "Core Spark operations demonstrated:\n",
        "\n",
        "select()\n",
        "\n",
        "withColumn()\n",
        "\n",
        "filter()\n",
        "\n",
        "groupBy()\n",
        "\n",
        "agg()\n",
        "\n",
        "These transformations use Spark SQL’s Catalyst optimizer, making them more efficient than equivalent RDD operations."
      ],
      "metadata": {
        "id": "Xn7wB4YRefv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customers.createOrReplaceTempView(\"customers\")\n",
        "products.createOrReplaceTempView(\"products\")\n",
        "orders.createOrReplaceTempView(\"orders\")\n",
        "fact_orders.createOrReplaceTempView(\"fact_orders\")\n"
      ],
      "metadata": {
        "id": "twcNd7BIitY8"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_products_sql = spark.sql(\"\"\"\n",
        "SELECT\n",
        "    product_id,\n",
        "    SUM(quantity) AS total_units_sold,\n",
        "    SUM(order_value) AS total_revenue\n",
        "FROM fact_orders\n",
        "GROUP BY product_id\n",
        "ORDER BY total_revenue DESC\n",
        "\"\"\")\n",
        "\n",
        "top_products_sql.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTv_pZzciuwx",
        "outputId": "b35640b9-eb06-4d96-9806-8b9ffd91d5de"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------------+-------------+\n",
            "|product_id|total_units_sold|total_revenue|\n",
            "+----------+----------------+-------------+\n",
            "|      P001|               1|       1200.5|\n",
            "|      P004|               3|        661.2|\n",
            "|      P003|               1|       300.75|\n",
            "|      P002|               2|        300.0|\n",
            "+----------+----------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yearly_revenue_sql = spark.sql(\"\"\"\n",
        "SELECT\n",
        "    order_year,\n",
        "    SUM(order_value) AS yearly_revenue,\n",
        "    COUNT(DISTINCT order_id) AS order_count\n",
        "FROM fact_orders\n",
        "GROUP BY order_year\n",
        "ORDER BY order_year\n",
        "\"\"\")\n",
        "\n",
        "yearly_revenue_sql.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlyEKelPiyGk",
        "outputId": "c930c5e3-fc67-4e18-bb6e-b67a029b268b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------+-----------+\n",
            "|order_year|yearly_revenue|order_count|\n",
            "+----------+--------------+-----------+\n",
            "|      2022|         661.2|          1|\n",
            "|      2023|       1801.25|          3|\n",
            "+----------+--------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "repeat_customers_sql = spark.sql(\"\"\"\n",
        "SELECT\n",
        "    customer_id,\n",
        "    COUNT(DISTINCT order_id) AS number_of_orders,\n",
        "    SUM(order_value) AS lifetime_value\n",
        "FROM fact_orders\n",
        "GROUP BY customer_id\n",
        "HAVING COUNT(DISTINCT order_id) > 1\n",
        "ORDER BY lifetime_value DESC\n",
        "\"\"\")\n",
        "\n",
        "repeat_customers_sql.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJQSN49bi4-K",
        "outputId": "6b67348c-481d-42d9-ecff-7304f4b97b2d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------------+--------------+\n",
            "|customer_id|number_of_orders|lifetime_value|\n",
            "+-----------+----------------+--------------+\n",
            "|       C001|               2|       1501.25|\n",
            "+-----------+----------------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yearly_revenue_sql.explain(True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHVM68oTi8uu",
        "outputId": "8d9b1cd2-0735-4638-fad4-e583940ca1ac"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Sort ['order_year ASC NULLS FIRST], true\n",
            "+- 'Aggregate ['order_year], ['order_year, 'SUM('order_value) AS yearly_revenue#768, 'COUNT(distinct 'order_id) AS order_count#769]\n",
            "   +- 'UnresolvedRelation [fact_orders], [], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "order_year: int, yearly_revenue: double, order_count: bigint\n",
            "Sort [order_year#182 ASC NULLS FIRST], true\n",
            "+- Aggregate [order_year#182], [order_year#182, sum(order_value#180) AS yearly_revenue#768, count(distinct order_id#19) AS order_count#769L]\n",
            "   +- SubqueryAlias fact_orders\n",
            "      +- View (`fact_orders`, [product_id#21, order_id#19, customer_id#20, quantity#103, order_ts#104, platform#24, product_name#16, category#17, unit_price#102, order_value#180, is_high_value_order#181, order_year#182])\n",
            "         +- Project [product_id#21, order_id#19, customer_id#20, quantity#103, order_ts#104, platform#24, product_name#16, category#17, unit_price#102, order_value#180, is_high_value_order#181, year(cast(order_ts#104 as date)) AS order_year#182]\n",
            "            +- Project [product_id#21, order_id#19, customer_id#20, quantity#103, order_ts#104, platform#24, product_name#16, category#17, unit_price#102, order_value#180, (order_value#180 >= cast(500 as double)) AS is_high_value_order#181]\n",
            "               +- Project [product_id#21, order_id#19, customer_id#20, quantity#103, order_ts#104, platform#24, product_name#16, category#17, unit_price#102, (cast(quantity#103 as double) * unit_price#102) AS order_value#180]\n",
            "                  +- Project [product_id#21, order_id#19, customer_id#20, quantity#103, order_ts#104, platform#24, product_name#16, category#17, unit_price#102]\n",
            "                     +- Join LeftOuter, (product_id#21 = product_id#15)\n",
            "                        :- SubqueryAlias o\n",
            "                        :  +- Deduplicate [order_id#19]\n",
            "                        :     +- Filter (quantity#103 > 0)\n",
            "                        :        +- Project [order_id#19, customer_id#20, product_id#21, quantity#103, to_timestamp(order_ts#23, None, TimestampType, Some(Etc/UTC), true) AS order_ts#104, platform#24]\n",
            "                        :           +- Project [order_id#19, customer_id#20, product_id#21, cast(quantity#22 as int) AS quantity#103, order_ts#23, platform#24]\n",
            "                        :              +- Filter isnotnull(product_id#21)\n",
            "                        :                 +- Filter isnotnull(customer_id#20)\n",
            "                        :                    +- Filter isnotnull(order_id#19)\n",
            "                        :                       +- Relation [order_id#19,customer_id#20,product_id#21,quantity#22,order_ts#23,platform#24] json\n",
            "                        +- SubqueryAlias p\n",
            "                           +- Deduplicate [product_id#15]\n",
            "                              +- Project [product_id#15, product_name#16, category#17, cast(unit_price#18 as double) AS unit_price#102]\n",
            "                                 +- Filter isnotnull(product_id#15)\n",
            "                                    +- Relation [product_id#15,product_name#16,category#17,unit_price#18] csv\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Sort [order_year#182 ASC NULLS FIRST], true\n",
            "+- Aggregate [order_year#182], [order_year#182, sum(order_value#180) AS yearly_revenue#768, count(distinct order_id#19) AS order_count#769L]\n",
            "   +- Project [order_id#19, (cast(quantity#1005 as double) * unit_price#1015) AS order_value#180, year(cast(order_ts#1007 as date)) AS order_year#182]\n",
            "      +- Join LeftOuter, (product_id#1003 = product_id#15)\n",
            "         :- Aggregate [order_id#19], [order_id#19, first(product_id#21, false) AS product_id#1003, first(quantity#103, false) AS quantity#1005, first(order_ts#104, false) AS order_ts#1007]\n",
            "         :  +- Project [order_id#19, product_id#21, cast(quantity#22 as int) AS quantity#103, cast(order_ts#23 as timestamp) AS order_ts#104]\n",
            "         :     +- Filter (isnotnull(quantity#22) AND ((isnotnull(order_id#19) AND isnotnull(customer_id#20)) AND (isnotnull(product_id#21) AND (cast(quantity#22 as int) > 0))))\n",
            "         :        +- Relation [order_id#19,customer_id#20,product_id#21,quantity#22,order_ts#23,platform#24] json\n",
            "         +- Aggregate [product_id#15], [product_id#15, first(unit_price#102, false) AS unit_price#1015]\n",
            "            +- Project [product_id#15, cast(unit_price#18 as double) AS unit_price#102]\n",
            "               +- Filter isnotnull(product_id#15)\n",
            "                  +- Relation [product_id#15,product_name#16,category#17,unit_price#18] csv\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Sort [order_year#182 ASC NULLS FIRST], true, 0\n",
            "   +- Exchange rangepartitioning(order_year#182 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=2418]\n",
            "      +- HashAggregate(keys=[order_year#182], functions=[sum(order_value#180), count(distinct order_id#19)], output=[order_year#182, yearly_revenue#768, order_count#769L])\n",
            "         +- Exchange hashpartitioning(order_year#182, 200), ENSURE_REQUIREMENTS, [plan_id=2415]\n",
            "            +- HashAggregate(keys=[order_year#182], functions=[merge_sum(order_value#180), partial_count(distinct order_id#19)], output=[order_year#182, sum#792, count#795L])\n",
            "               +- HashAggregate(keys=[order_year#182, order_id#19], functions=[merge_sum(order_value#180)], output=[order_year#182, order_id#19, sum#792])\n",
            "                  +- HashAggregate(keys=[order_year#182, order_id#19], functions=[partial_sum(order_value#180)], output=[order_year#182, order_id#19, sum#792])\n",
            "                     +- Project [order_id#19, (cast(quantity#1005 as double) * unit_price#1015) AS order_value#180, year(cast(order_ts#1007 as date)) AS order_year#182]\n",
            "                        +- BroadcastHashJoin [product_id#1003], [product_id#15], LeftOuter, BuildRight, false\n",
            "                           :- SortAggregate(key=[order_id#19], functions=[first(product_id#21, false), first(quantity#103, false), first(order_ts#104, false)], output=[order_id#19, product_id#1003, quantity#1005, order_ts#1007])\n",
            "                           :  +- Sort [order_id#19 ASC NULLS FIRST], false, 0\n",
            "                           :     +- Exchange hashpartitioning(order_id#19, 200), ENSURE_REQUIREMENTS, [plan_id=2402]\n",
            "                           :        +- SortAggregate(key=[order_id#19], functions=[partial_first(product_id#21, false), partial_first(quantity#103, false), partial_first(order_ts#104, false)], output=[order_id#19, first#1024, valueSet#1025, first#1026, valueSet#1027, first#1028, valueSet#1029])\n",
            "                           :           +- Sort [order_id#19 ASC NULLS FIRST], false, 0\n",
            "                           :              +- Project [order_id#19, product_id#21, cast(quantity#22 as int) AS quantity#103, cast(order_ts#23 as timestamp) AS order_ts#104]\n",
            "                           :                 +- Filter ((((isnotnull(quantity#22) AND isnotnull(order_id#19)) AND isnotnull(customer_id#20)) AND isnotnull(product_id#21)) AND (cast(quantity#22 as int) > 0))\n",
            "                           :                    +- FileScan json [order_id#19,customer_id#20,product_id#21,quantity#22,order_ts#23] Batched: false, DataFilters: [isnotnull(quantity#22), isnotnull(order_id#19), isnotnull(customer_id#20), isnotnull(product_id#..., Format: JSON, Location: InMemoryFileIndex(1 paths)[file:/content/sparkflow_raw/orders.json], PartitionFilters: [], PushedFilters: [IsNotNull(quantity), IsNotNull(order_id), IsNotNull(customer_id), IsNotNull(product_id)], ReadSchema: struct<order_id:string,customer_id:string,product_id:string,quantity:string,order_ts:string>\n",
            "                           +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=2408]\n",
            "                              +- HashAggregate(keys=[product_id#15], functions=[first(unit_price#102, false)], output=[product_id#15, unit_price#1015])\n",
            "                                 +- Exchange hashpartitioning(product_id#15, 200), ENSURE_REQUIREMENTS, [plan_id=2405]\n",
            "                                    +- HashAggregate(keys=[product_id#15], functions=[partial_first(unit_price#102, false)], output=[product_id#15, first#1032, valueSet#1033])\n",
            "                                       +- Project [product_id#15, cast(unit_price#18 as double) AS unit_price#102]\n",
            "                                          +- Filter isnotnull(product_id#15)\n",
            "                                             +- FileScan csv [product_id#15,unit_price#18] Batched: false, DataFilters: [isnotnull(product_id#15)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/sparkflow_raw/products.csv], PartitionFilters: [], PushedFilters: [IsNotNull(product_id)], ReadSchema: struct<product_id:string,unit_price:string>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 6 – Spark SQL Analytics\n",
        "\n",
        "In this stage, cleaned DataFrames were registered as temporary views using createOrReplaceTempView().\n",
        "\n",
        "Spark SQL was then used to perform analytical queries including:\n",
        "\n",
        "Top-selling products by revenue\n",
        "\n",
        "Yearly revenue aggregation\n",
        "\n",
        "Identification of repeat customers\n",
        "\n",
        "Spark SQL leverages the Catalyst Optimizer to transform SQL queries into efficient execution plans. Although the same results can be achieved using DataFrame APIs, SQL improves readability and is often preferred in collaborative data engineering environments where analysts and engineers share logic.\n",
        "\n",
        "Execution plans were examined using .explain(True) to validate query optimization and stage planning."
      ],
      "metadata": {
        "id": "TlUcHgBtjHls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_path = \"/content/sparkflow_output\"\n"
      ],
      "metadata": {
        "id": "YR8Zsi4VjJlV"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(\n",
        "    yearly_revenue_sql\n",
        "    .coalesce(1)   # small dataset only; never do this on large production data\n",
        "    .write\n",
        "    .mode(\"overwrite\")   # overwrite because this is a batch rebuild\n",
        "    .option(\"header\", \"true\")\n",
        "    .csv(f\"{output_path}/analytics/yearly_revenue_csv\")\n",
        ")\n"
      ],
      "metadata": {
        "id": "cymzFasqjTgy"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(\n",
        "    fact_orders\n",
        "    .write\n",
        "    .mode(\"overwrite\")   # full batch overwrite\n",
        "    .partitionBy(\"order_year\")\n",
        "    .parquet(f\"{output_path}/warehouse/fact_orders_parquet\")\n",
        ")\n"
      ],
      "metadata": {
        "id": "bqfrWeyejYtX"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "base = \"/content/sparkflow_output/warehouse/fact_orders_parquet\"\n",
        "\n",
        "for root, dirs, files in os.walk(base):\n",
        "    print(f\"\\nFolder: {root}\")\n",
        "    for d in dirs:\n",
        "        print(\"  Subfolder:\", d)\n",
        "    for f in files:\n",
        "        print(\"  File:\", f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubgRjDvNjbsF",
        "outputId": "bf534908-d606-4337-88a2-1150f6d97e9f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Folder: /content/sparkflow_output/warehouse/fact_orders_parquet\n",
            "  Subfolder: order_year=2022\n",
            "  Subfolder: order_year=2023\n",
            "  File: _SUCCESS\n",
            "  File: ._SUCCESS.crc\n",
            "\n",
            "Folder: /content/sparkflow_output/warehouse/fact_orders_parquet/order_year=2022\n",
            "  File: part-00000-bb3a7c3a-2fde-45cf-aee8-66f1b436d67a.c000.snappy.parquet\n",
            "  File: .part-00000-bb3a7c3a-2fde-45cf-aee8-66f1b436d67a.c000.snappy.parquet.crc\n",
            "\n",
            "Folder: /content/sparkflow_output/warehouse/fact_orders_parquet/order_year=2023\n",
            "  File: part-00000-bb3a7c3a-2fde-45cf-aee8-66f1b436d67a.c000.snappy.parquet\n",
            "  File: .part-00000-bb3a7c3a-2fde-45cf-aee8-66f1b436d67a.c000.snappy.parquet.crc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parquet_test = spark.read.parquet(\"/content/sparkflow_output/warehouse/fact_orders_parquet\")\n"
      ],
      "metadata": {
        "id": "6-hWHZS0j9Mr"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parquet_test.select(\"order_year\").distinct().show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1QowKTmkCIz",
        "outputId": "cacb9687-df74-4b3d-b5d3-dba8b2778af3"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|order_year|\n",
            "+----------+\n",
            "|      2023|\n",
            "|      2022|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "customers.write.mode(\"overwrite\").parquet(f\"{output_path}/warehouse/dim_customers\")\n",
        "products.write.mode(\"overwrite\").parquet(f\"{output_path}/warehouse/dim_products\")\n"
      ],
      "metadata": {
        "id": "kO-at7cdkKei"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "print(\"Analytics files:\")\n",
        "print(os.listdir(f\"{output_path}/analytics/yearly_revenue_csv\"))\n",
        "\n",
        "print(\"\\nWarehouse contents:\")\n",
        "print(os.listdir(f\"{output_path}/warehouse/fact_orders_parquet\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wQggjlekPEQ",
        "outputId": "8b56916f-a163-425b-985a-fb620d5b9453"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analytics files:\n",
            "['_SUCCESS', 'part-00000-d26bda5f-201a-46f4-a921-24300e3b89c0-c000.csv', '._SUCCESS.crc', '.part-00000-d26bda5f-201a-46f4-a921-24300e3b89c0-c000.csv.crc']\n",
            "\n",
            "Warehouse contents:\n",
            "['order_year=2022', '_SUCCESS', 'order_year=2023', '._SUCCESS.crc']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Why does Spark outperform traditional Python for large datasets?\n",
        "\n",
        "Spark distributes data and computation across multiple cores or machines, allowing parallel processing. Traditional Python (e.g., Pandas) runs in a single process and is limited by memory and CPU of one machine. Spark also uses optimized execution engines (Catalyst and Tungsten), minimizes disk IO through in-memory computation, and reduces Python interpreter overhead by executing operations in JVM space. This makes Spark scalable for large datasets that exceed single-machine capacity.\n",
        "\n",
        "2. Why is SparkSession considered the unified entry point in modern Spark applications?\n",
        "\n",
        "SparkSession unifies previously separate contexts (SparkContext, SQLContext, HiveContext) into a single API. It manages configuration, DataFrame creation, SQL execution, catalog access, and integration with other Spark components. This simplifies application design and reduces fragmentation across APIs.\n",
        "\n",
        "3. What would happen if the Driver fails during execution?\n",
        "\n",
        "If the Driver fails, the Spark application fails because the Driver maintains the execution plan, schedules tasks, and coordinates executors. Executors cannot continue independently since they depend on the Driver for instructions. In production clusters, driver recovery mechanisms may exist, but typically a driver failure terminates the job.\n",
        "\n",
        "4. Why is schema inference risky at scale?\n",
        "\n",
        "Schema inference requires scanning data to detect types, which increases read time. Inconsistent or malformed records can lead to incorrect type detection. In distributed environments, evolving schemas across partitions can cause runtime errors. Explicit schema definition ensures predictable and stable behavior.\n",
        "\n",
        "5. Why are RDDs considered more powerful but less commonly used than DataFrames?\n",
        "\n",
        "RDDs allow arbitrary transformations and full control over distributed logic. However, they lack query optimization, type safety, and performance enhancements provided by Spark SQL. DataFrames are optimized by Catalyst and Tungsten, making them faster and more efficient for structured data processing. Therefore, DataFrames are preferred in most production workloads.\n",
        "\n",
        "6. Why are Spark transformations considered lazy, and how does this improve performance?\n",
        "\n",
        "Transformations in Spark are lazy, meaning they are not executed immediately. Instead, Spark builds a logical execution plan (DAG). Execution occurs only when an action is called. This allows Spark to optimize the entire plan, combine operations, eliminate unnecessary work, and reduce shuffles before execution.\n",
        "\n",
        "7. When would Spark SQL be preferred over DataFrame APIs in a team environment?\n",
        "\n",
        "Spark SQL is often preferred when teams include analysts and data engineers familiar with SQL. SQL queries are easier to read, review, and standardize across teams. It also allows clearer separation between business logic and application code.\n",
        "\n",
        "8. Why is Parquet preferred in data lakes and medallion architectures?\n",
        "\n",
        "Parquet is a columnar storage format that supports compression and predicate pushdown. It reduces IO by reading only required columns and improves query performance. Partitioning combined with Parquet enables efficient large-scale analytical processing.\n",
        "\n",
        "9. How could this pipeline be extended for real-time data ingestion?\n",
        "\n",
        "The pipeline could be extended using Structured Streaming. Instead of reading batch files, Spark could consume data from Kafka or event streams, apply the same transformations, and write incremental updates to partitioned Parquet or Delta tables with checkpointing for fault tolerance.\n",
        "\n",
        "10. What trade-offs exist between RDDs and DataFrames?\n",
        "\n",
        "RDDs provide maximum flexibility but lack automatic optimization and are generally slower for structured data. DataFrames offer high performance, optimization, and ease of use but restrict operations to structured schemas. In most modern applications, DataFrames are preferred unless low-level control is required.\n",
        "\n",
        "11. Why is distributed computing harder to debug?\n",
        "\n",
        "Failures can occur across multiple executors, making logs distributed and harder to trace. Data skew, shuffle issues, and serialization errors can occur unpredictably. Errors may not surface until large-scale execution, making debugging more complex than single-machine programs.\n",
        "\n",
        "12. How does Spark enforce fault tolerance?\n",
        "\n",
        "Spark tracks lineage information for RDDs and DataFrames. If a partition is lost, Spark can recompute it using the transformation history. Tasks are retried on failure, and shuffle data is managed to ensure recovery when possible.\n",
        "\n",
        "13. What mistakes commonly cause Spark jobs to fail in production?\n",
        "\n",
        "Common failures include:\n",
        "\n",
        "Using collect() or toPandas() on large datasets (driver memory overflow)\n",
        "\n",
        "Poor partitioning leading to data skew\n",
        "\n",
        "Excessive small files\n",
        "\n",
        "Broadcasting large datasets incorrectly\n",
        "\n",
        "Ignoring schema drift\n",
        "\n",
        "Lack of checkpointing in streaming jobs\n",
        "\n",
        "Improper resource configuration"
      ],
      "metadata": {
        "id": "zZ78Thbfkvmj"
      }
    }
  ]
}